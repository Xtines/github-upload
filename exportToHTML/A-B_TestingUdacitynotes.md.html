<html>
<head>
<title>A-B_TestingUdacitynotes.md</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.ln { color: #999999; font-weight: normal; font-style: normal; }
.s0 { color: rgb(0,0,0); }
.s1 { color: rgb(128,128,128); font-style: italic; }
.s2 { color: rgb(0,0,128); font-weight: bold; }
.s3 { color: rgb(0,0,0); font-style: italic; }
</style>
</head>
<BODY BGCOLOR="#ffffff">
<TABLE CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#c0c0c0" >
<TR><TD><CENTER>
<FONT FACE="Arial, Helvetica" COLOR="#000000">
A-B_TestingUdacitynotes.md</FONT>
</center></TD></TR></TABLE>
<pre>
<span class="s0"> 
</span><span class="s1">---</span><span class="s0"> 
layout: post 
title:  &quot;A/B Testing - Udacity course notes&quot; 
date:   2017-03-13 
categories: markdown notes  
output: html_document 
 
</span><span class="s1">---</span><span class="s0"> 
 
 
A/B Testing course from Udacity, built by Google 
</span><span class="s2">====================</span><span class="s0"> 
 
Course Overview - 4 sections 
</span><span class="s2">------------------------------</span><span class="s0"> 
 
</span><span class="s2">### </span><span class="s0">Syllabus 
 
The syllabus for this online class can be found in this [Udacity course link][Udacity on A/B Test].  
This course will cover the design and analysis of A/B tests, also known as split tests, which are online experiments used to test potential improvements to a website or mobile application. Two versions of the website are shown to different users - usually the existing website and a potential change. Then, the results are analyzed to determine whether the change is an improvement worth launching. This course will cover how to choose and characterize metrics to evaluate your experiments, how to design an experiment with enough statistical power, how to analyze the results and draw valid conclusions, and how to ensure that the the participants of your experiments are adequately protected. 
 
 
</span><span class="s2">### </span><span class="s0">Overview of A/B Testing 
**Overview of Business Example:** Audacity, an education provider, creates online finance courses. Considering their customer funnel, we'll consider a change in one of their website features, such as the &quot;Start&quot; button.   
 
Steps in the evaluation process - 
 
</span><span class="s2">####</span><span class="s0">Metric Choice 
Two metrics: 
 
</span><span class="s2">* </span><span class="s0">x **CTR%** = no. of clicks/no. of page views 
</span><span class="s2">* </span><span class="s0">-&gt; **Click-through-probability (CTP%)** = unique visitors who click/unique page visitors. Count at most one child-click per page view.  
 
 
*Why is the *Click-through-probability* preferred?* 
Because we do not want to count/consider the no. of times users double-clicked or reloaded a page, we want only want to understand the probability of users who would advance to the next level of the process.  
 
 
</span><span class="s2">1. </span><span class="s0">**Which Distribution?**  
We expect the Click-through probability to follow a *Binomial distribution* 
 
 
</span><span class="s2">2. </span><span class="s0">**Confidence Intervals**  
Calculating a CI: 
    </span><span class="s2">*  </span><span class="s0">rule of thumb: if N * p-hat &gt; 5, then it's safe to assume the distribution to be approx. normal 
    </span><span class="s2">*  </span><span class="s0">margin of error = z * √(p-hat*(1 - p-hat)/n) 
​ 
 
</span><span class="s2">3. </span><span class="s0">**Hypothesis Testing, or Establishing Statistical Signifiance** - A way to establish how likely it is your results occurred by chance.  
 
*Hypothesis Testing steps* 
 
</span><span class="s2">* </span><span class="s0">Null hypo Ho: p-control = p-exposed 
</span><span class="s2">* </span><span class="s0">Alternative hypo Ha:   p-control - p-exposed not equal 0.  
</span><span class="s2">* </span><span class="s0">Calculate Prob(p-exposed - p-control | Ho), which tells us the difference we observed could have occurred by chance, or if it would be very unlikely to have occurred if the null hypothesis were true.  
 
</span><span class="s2">* </span><span class="s0">Choose a cut-off threshold, called Alpha, to determine... 
 
 
 
</span><span class="s2">#### </span><span class="s0">**Comparing Two Samples**  
 
**Pooled Standard Error -** 
 
</span><span class="s2">* </span><span class="s0">**Pooled probability, phat-pool** of a click across groups= p-hat(pooled) = (Xcont + Xexp)/(Ncont + Nexp) 
 
</span><span class="s2">* </span><span class="s0">**SE-pool** = √(phat-pool * (1-phat-pool)*(1/Ncont + 1/Nexp)) 
 
</span><span class="s2">* </span><span class="s0">**difference d**= phat-exp - phat-cont 
d ~ N(0, SEpool) 
Ho: d = 0 
Ha:  reject null if d &gt; 2 *SEpool, d &lt; -2*SEpool 
 
 
**Practically Significant, or Substantive** 
What's next? Is the observed change in click-through probability practically significant?  
What size change matters enough to us? 
 
</span><span class="s2">- </span><span class="s0">Rule of thumb: a 1-2% change in click-through probability is quite large even at Google.  
</span><span class="s2">- </span><span class="s0">What you want to observe: repeatability, which is what statistically significant is about. Want to size your experiment so that the stat significance bar is lower than the practical significance bar. 
</span><span class="s2">- </span><span class="s0">Can be fair to assume from a business perspective, a 2% change in click-through probability would be practically significant.  
 
 
</span><span class="s2">#### </span><span class="s0">Designing the experiment  
This is where we need to figure out *how many* page views we need in order to get a statistically significant result to measure the change in CTP%.  
 
**Statistical power** defined: 
 
</span><span class="s2">* </span><span class="s0">power has an inverse trade-off with sample size - the smaller the change you want to measure, the larger the size needed (in other words, more page views in your control and exposed groups) 
 
 
**How many page views or N?** 
 
</span><span class="s2">* </span><span class="s0">alpha = P(rejecting null | Ho true); &quot;he probability you are willing to accept for **incorrectly concluding that your improvement was successful**, even though it was not.&quot;  
 
</span><span class="s2">* </span><span class="s0">beta B = P(fail to reject| Ho false), or the **probability of falsely failing to conclude there is a measured diff**, depends on how large your effect really was.  
-&gt; means you are likely to fail to design an experiment that actually did have a measured difference in response rates.  
 
*ex)* larger measured differences will have a lower beta, that is, a lower chance of error. 
 
</span><span class="s2">* </span><span class="s0">Confidence level = 1 - alpha 
</span><span class="s2">* </span><span class="s0">1-beta = statistical power/sensitivity (often ~80%) -&gt; in general, you'd want your experiment to have a high level of sensitivity. 
 
** the need to balance alpha &amp; beta:**  
 
 
 
 
</span><span class="s1">****</span><span class="s0"> 
 
</span><span class="s2">###</span><span class="s0">Section title 
 
 
</span><span class="s2">#### </span><span class="s0">Subsection title 
Text text text 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
</span><span class="s2">[Udacity on A/B Test]</span><span class="s0">: </span><span class="s3">https://de.udacity.com/course/ab-testing--ud257/</span></pre>
</body>
</html>